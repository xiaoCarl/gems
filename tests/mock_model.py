"""
æ¨¡æ‹ŸLLMæ¨¡å‹

æä¾›æ¨¡æ‹Ÿçš„LLMè°ƒç”¨ï¼Œç”¨äºåœ¨æ²¡æœ‰APIå¯†é’¥çš„æƒ…å†µä¸‹è¿›è¡Œæµ‹è¯•ã€‚
"""

import time
from typing import Any

from langchain_core.messages import AIMessage
from langchain_core.tools import BaseTool
from pydantic import BaseModel


class MockLLM:
    """æ¨¡æ‹ŸLLMç±»"""

    def __init__(self):
        self.model = "mock-deepseek-chat"
        self.temperature = 0

    def with_structured_output(
        self, output_schema: type[BaseModel], method: str = "function_calling"
    ):
        """æ¨¡æ‹Ÿç»“æ„åŒ–è¾“å‡º"""
        return self

    def bind_tools(self, tools: list[BaseTool]):
        """æ¨¡æ‹Ÿå·¥å…·ç»‘å®š"""
        return self

    def invoke(self, input_data: dict) -> Any:
        """æ¨¡æ‹Ÿè°ƒç”¨"""
        prompt = input_data.get("prompt", "")

        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.5)

        # æ ¹æ®æç¤ºå†…å®¹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿå“åº”
        if "plan" in prompt.lower() or "task" in prompt.lower():
            # æ¨¡æ‹Ÿä»»åŠ¡è§„åˆ’å“åº”
            return {
                "tasks": [
                    {"id": 1, "description": "è·å–å…¬å¸åŸºæœ¬ä¿¡æ¯", "done": False},
                    {"id": 2, "description": "åˆ†æè´¢åŠ¡æ•°æ®", "done": False},
                    {"id": 3, "description": "è¯„ä¼°ä¼°å€¼æ°´å¹³", "done": False},
                ]
            }
        elif "done" in prompt.lower():
            # æ¨¡æ‹Ÿä»»åŠ¡å®Œæˆæ£€æŸ¥
            return {"done": True}
        elif "optimize" in prompt.lower():
            # æ¨¡æ‹Ÿå‚æ•°ä¼˜åŒ–
            return {"arguments": {"symbol": "AAPL", "period": "annual", "limit": 5}}
        elif "answer" in prompt.lower() or "analysis" in prompt.lower():
            # æ¨¡æ‹Ÿä»·å€¼æŠ•èµ„åˆ†æå“åº”
            return {
                "answer": """
# ä»·å€¼æŠ•èµ„åˆ†ææŠ¥å‘Š - è‹¹æœå…¬å¸ (AAPL)

## 1. å¥½ç”Ÿæ„ (Good Business)

### æŠ¤åŸæ²³åˆ†æ
- **å“ç‰ŒæŠ¤åŸæ²³**: æå¼º - è‹¹æœæ˜¯å…¨çƒæœ€å…·ä»·å€¼çš„å“ç‰Œä¹‹ä¸€
- **ç”Ÿæ€ç³»ç»ŸæŠ¤åŸæ²³**: æå¼º - iOSç”Ÿæ€ç³»ç»Ÿå½¢æˆå¼ºå¤§é”å®šæ•ˆåº”
- **æŠ€æœ¯æŠ¤åŸæ²³**: å¼º - èŠ¯ç‰‡è®¾è®¡å’Œè½¯ä»¶é›†æˆèƒ½åŠ›

### ç®¡ç†å±‚è´¨é‡
- è’‚å§†Â·åº“å…‹é¢†å¯¼å›¢é˜Ÿç¨³å®šï¼Œæ‰§è¡ŒåŠ›å¼º
- åˆ›æ–°æ–‡åŒ–æŒç»­ï¼Œä½†äº§å“è¿­ä»£é€Ÿåº¦æœ‰æ‰€æ”¾ç¼“

### ä¸šåŠ¡ç®€å•æ˜“æ‡‚
- ä¸»è¦æ”¶å…¥æ¥æºæ¸…æ™°ï¼šiPhoneã€æœåŠ¡ã€Macã€iPad
- å•†ä¸šæ¨¡å¼ç®€å•ï¼šç¡¬ä»¶é”€å”® + æœåŠ¡è®¢é˜…

### è‡ªç”±ç°é‡‘æµ
- 2023å¹´è‡ªç”±ç°é‡‘æµï¼š$1000äº¿ç¾å…ƒ
- ç°é‡‘æµç”Ÿæˆèƒ½åŠ›æå¼ºï¼ŒæŒç»­å›è´­å’Œåˆ†çº¢

## 2. å¥½ä»·æ ¼ (Good Price)

### å¸‚ç›ˆç‡ä¼°å€¼
- å½“å‰PEï¼š28å€
- å†å²PEåŒºé—´ï¼š10-35å€
- ç›¸å¯¹äºå†å²ä¼°å€¼å¤„äºä¸­é«˜ä½

### å¸‚å‡€ç‡ä¼°å€¼
- å½“å‰PBï¼š35å€
- èµ„äº§è´¨é‡ä¼˜ç§€ï¼Œä½†ä¼°å€¼åé«˜

### èµ„æœ¬å›æŠ¥ç‡
- ROEï¼š147%
- ROICï¼š57%
- èµ„æœ¬é…ç½®æ•ˆç‡æé«˜

### å®‰å…¨è¾¹é™…
- å½“å‰ä»·æ ¼å®‰å…¨è¾¹é™…ï¼šä¸­ç­‰åä½
- å»ºè®®ç­‰å¾…æ›´å¥½çš„ä¹°å…¥æ—¶æœº

## æŠ•èµ„å»ºè®®

**æ€»ä½“è¯„ä¼°**: ä¼˜ç§€çš„ä¼ä¸šï¼Œä½†å½“å‰ä¼°å€¼åé«˜

**å»ºè®®æ“ä½œ**:
- é•¿æœŸæŒæœ‰è€…å¯ç»§ç»­æŒæœ‰
- æ–°æŠ•èµ„è€…å»ºè®®ç­‰å¾…å›è°ƒè‡³PE 20-25å€åŒºé—´
- å»ºè®®ä»“ä½ï¼šç°æœ‰æŒä»“ç»´æŒï¼Œæ–°å¢ä»“ä½ç­‰å¾…

**å…³é”®é£é™©**:
- æ™ºèƒ½æ‰‹æœºå¸‚åœºé¥±å’Œ
- ç›‘ç®¡é£é™©å¢åŠ 
- åˆ›æ–°é€Ÿåº¦æ”¾ç¼“
"""
            }
        else:
            # é»˜è®¤æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨å“åº”
            return AIMessage(
                content="åˆ†æè¿›è¡Œä¸­...",
                tool_calls=[
                    {
                        "id": "call_001",
                        "name": "get_company_info",
                        "args": {"symbol": "AAPL"},
                    }
                ],
            )


# åˆ›å»ºæ¨¡æ‹ŸLLMå®ä¾‹
mock_llm = MockLLM()


def call_llm(
    prompt: str,
    system_prompt: str | None = None,
    output_schema: type[BaseModel] | None = None,
    tools: list[BaseTool] | None = None,
) -> Any:
    """
    æ¨¡æ‹ŸLLMè°ƒç”¨å‡½æ•°

    åœ¨æ²¡æœ‰çœŸå®APIå¯†é’¥çš„æƒ…å†µä¸‹æä¾›æ¨¡æ‹Ÿå“åº”ã€‚
    """
    print("ğŸ”§ ä½¿ç”¨æ¨¡æ‹ŸLLMè¿›è¡Œæµ‹è¯•...")

    # ä½¿ç”¨æ¨¡æ‹ŸLLM
    runnable = mock_llm

    if output_schema:
        runnable = mock_llm.with_structured_output(output_schema)
    elif tools:
        runnable = mock_llm.bind_tools(tools)

    # æ¨¡æ‹Ÿè°ƒç”¨
    result = runnable.invoke({"prompt": prompt})

    return result
